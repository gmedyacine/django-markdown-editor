Got it 👍 Here’s the English translation of my previous answer:

---

I can see the error message:

```
RuntimeError: DataLoader worker (pid 24459) is killed by signal: Bus error.  
It is possible that dataloader's workers are out of shared memory.  
Please try to raise your shared memory limit.
```

---

### 👉 Why this happens

* When using **PyTorch DataLoader** with multiple `num_workers`, each worker loads batches in parallel and stores data in shared memory (`/dev/shm`).
* By default, in a Docker container (like on Domino), the `/dev/shm` size is only **64 MB**, which is too small for large datasets.
* As a result: bus error → workers are killed.

---

### ✅ Possible solutions

#### 1. Increase shared memory (`/dev/shm`)

* If you control the container launch:

  ```bash
  docker run --ipc=host ...
  ```

  or

  ```bash
  docker run --shm-size=2g ...
  ```
* On Domino, it depends on how the platform manages environments. You can:

  * Ask the admin to increase `--shm-size` in the Docker spec used by the environment.
  * Or check if Domino allows configuring shared memory in the compute environment (sometimes under “Shared Memory”).

#### 2. Reduce DataLoader memory usage

* In your PyTorch code:

  ```python
  DataLoader(dataset, num_workers=0)
  ```

  or lower the number of workers (e.g. `num_workers=2` instead of 8).
* If not required, disable `pin_memory=True`.
* Reduce the `batch_size`.

#### 3. Quick debug

* Test the DataLoader with `num_workers=0`. If it works → the issue is definitely shared memory.
* Then either fix the Docker config (proper solution) or adjust DataLoader parameters (workaround).

---

### 🚀 Recommendation for Domino

1. **Short term (quick fix):** use `num_workers=0` or lower the batch size, just to unblock your run.
2. **Medium term (proper fix):** ask your Infra/DevOps team to increase `/dev/shm` size in the Domino workload config (e.g., `--shm-size=2g`). This is the right solution if PyTorch is used frequently.

---

Do you want me to also draft a short message in English that you can send back to your colleagues (like a clean reply to the Teams/Slack thread)?
